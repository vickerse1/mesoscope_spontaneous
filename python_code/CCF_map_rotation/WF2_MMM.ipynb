{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Sep 14 14:13:34 2022\n",
    "\n",
    "@author: dmc lab\n",
    "\n",
    "ION Grad Bootcamp 2022\n",
    "Widefield Data Analysis\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import easygui\n",
    "import tifffile\n",
    "import glob\n",
    "from skimage import io\n",
    "from skimage.measure import block_reduce\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import subplots\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_multi_tif(tif_folder, n_channels=2, down_sample_factor= 2,dtype=np.uint8):\n",
    "    tif_filepaths = glob.glob(os.path.join(tif_folder + '\\*.tif'))\n",
    "    print('\\n' + str(len(tif_filepaths)) + ' file(s) to process')\n",
    "    frame = 0 \n",
    "    for i, image_path in enumerate(tif_filepaths):\n",
    "        im = io.imread(image_path) #cv2 might also work for this\n",
    "        im = np.array(im,dtype=dtype)\n",
    "        file_frames = int(im.shape[0]/down_sample_factor)\n",
    "        \n",
    "        if n_channels==2:\n",
    "            if i == 0:\n",
    "                wf_blue = np.ndarray([len(tif_filepaths)*file_frames, int(im.shape[1]/down_sample_factor), int(im.shape[2]/down_sample_factor)])\n",
    "                wf_green = np.ndarray([len(tif_filepaths)*file_frames, int(im.shape[1]/down_sample_factor), int(im.shape[2]/down_sample_factor)])\n",
    "                   \n",
    "            wf_green[frame:frame+file_frames]= np.atleast_3d(block_reduce(im[np.arange(1,len(im),2)],(1,down_sample_factor,down_sample_factor),func=np.mean))\n",
    "            if file_frames == im.shape[0]/down_sample_factor:\n",
    "                wf_blue[frame:frame+file_frames] = np.atleast_3d(block_reduce(im[np.arange(0,len(im),2)],(1,down_sample_factor,down_sample_factor),func=np.mean))\n",
    "            else:\n",
    "                wf_blue[frame:frame+file_frames+1] = np.atleast_3d(block_reduce(im[np.arange(0,len(im),2)],(1,down_sample_factor,down_sample_factor),func=np.mean))\n",
    "        frame = frame+file_frames\n",
    "        print('\\nFile ' + str(i+1) + ' of ' + str(len(tif_filepaths)) + ' complete')\n",
    "    return wf_blue, wf_green"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESS WIDEFIELD RECORDINGS\n",
    "\n",
    "tif_folder = easygui.diropenbox() #change this if using hal or iq\n",
    "wf_blue, wf_green = import_multi_tif(tif_folder, n_channels=2, down_sample_factor= 2,dtype=np.float16)\n",
    "\n",
    "# add in trimming step HERE\n",
    "\n",
    "# CALCULATE DF/F \n",
    "uF_blue = wf_blue.reshape(wf_blue.shape[0], wf_blue.shape[1]*wf_blue.shape[2]) #unfold images so df/f calculation is easier and faster\n",
    "uF_green = wf_green.reshape(wf_green.shape[0], wf_green.shape[1]*wf_green.shape[2])\n",
    "\n",
    "base_blue = np.mean(uF_blue, axis = 0) #calculate base as mean across entire video\n",
    "base_green = np.median(uF_green, axis = 0)\n",
    "\n",
    "uF_dFF_blue = np.divide(uF_blue - base_blue, base_blue) # calculate df/f\n",
    "uF_dFF_green = np.divide(uF_green - base_green, base_green)\n",
    "\n",
    "dFF_blue = uF_dFF_blue.reshape(wf_blue.shape) # reshape to size of original image\n",
    "dFF_green = uF_dFF_green.reshape(wf_green.shape)\n",
    "\n",
    "\n",
    "# SAVE VIDEO\n",
    "os.chdir(tif_folder)\n",
    "tifffile.imwrite('dFF_blue.tif', dFF_blue.astype('float32'))\n",
    "tifffile.imwrite('dFF_green.tif', dFF_green.astype('float32'))\n",
    " \n",
    "\n",
    "# Open in ImageJ and view your recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESS AROUSAL MEASURES\n",
    "# BE SURE TO RUN TDMS TO TXT LABVIEW PROGRAM FIRST!!!!!!\n",
    "\n",
    "lv_folder = easygui.diropenbox()\n",
    "os.chdir(lv_folder)\n",
    "files_list = [] #pull out names of files and directories so you can go into LabView folder\n",
    "for root, directories, files in os.walk(lv_folder):\n",
    "   for name in files:\n",
    "      files_list.append(os.path.join(root, name))\n",
    "\n",
    "os.chdir(root[:-9])  # change directory to LabView folder\n",
    "frame_times = np.loadtxt('Frame_Clock_1.txt')\n",
    "mouseID = np.loadtxt('Animal_ID.txt', dtype = str)\n",
    "face_times = np.loadtxt('LV_FrameClock.txt')\n",
    "walk_times = np.loadtxt('Encoder_Time.txt')\n",
    "walk = np.loadtxt('Encoder_Value.txt')\n",
    "whisk = np.loadtxt('Whisker_Energy.txt')\n",
    "pupil = np.loadtxt('Pupil_Area.txt')\n",
    "aud_times = np.loadtxt('Aud_Stim_Time.txt') # THESE FILES WILL ONLY EXIST IF YOU RAN THAT MODALITY\n",
    "vis_times = np.loadtxt('Vis_Stim_Time.txt')\n",
    "\n",
    "avi_start = np.where(face_times>frame_times[0])[0][0]\n",
    "avi_end = np.where(face_times>frame_times[-1])[0][0]\n",
    "\n",
    "walk_start = np.where(walk_times>frame_times[0])[0][0]\n",
    "walk_end = np.where(walk_times>frame_times[-1])[0][0]\n",
    "\n",
    "walk = walk[walk_start:walk_end]\n",
    "pupil = pupil[avi_start:avi_end]\n",
    "whisk = whisk[avi_start:avi_end]\n",
    "\n",
    "plt.plot(pupil)\n",
    "plt.savefig('Spont_pupil')\n",
    "\n",
    "plt.plot(whisk)\n",
    "plt.savefig('Spont_whisk')\n",
    "\n",
    "plt.plot(walk)\n",
    "plt.savefig('Spont_walk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESS FACE VIDEO\n",
    "\n",
    "avi_folder = easygui.diropenbox()\n",
    "os.chdir(avi_folder)\n",
    "\n",
    "face_vid = io.imread('face_ds.tif')\n",
    "face_vid = np.array(face_vid,dtype=np.float16)\n",
    "\n",
    "face_vid = face_vid[avi_start:avi_end,:,:,0]\n",
    "face_vid_save = face_vid.astype(dtype = 'uint16')\n",
    "tifffile.imwrite('face_trim.tif',face_vid_save)\n",
    "\n",
    "# Now combine your face video, widefield recording, and arousal measures into a slide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRYING POINT CORRELATION\n",
    "\n",
    "\n",
    "aud_frames=[] # find frame numbers for all auditory stims\n",
    "for time in aud_times:\n",
    "    temp = np.where(frame_times>time)[0]\n",
    "    aud_frames = np.append(aud_frames,temp[0])\n",
    "\n",
    "#create 4D variable that is # of stims, # of frames to clip, x, y\n",
    "dFF_aud_clip = np.zeros((len(aud_frames), 60, int(dFF_blue.shape[1]),int(dFF_blue.shape[2]))) \n",
    "for i in range(len(aud_frames)):\n",
    "    temp = dFF_blue[int(aud_frames[i])+30:int(aud_frames[i])+90,:,:] # clip to 1 s ater to 3 s after\n",
    "    dFF_aud_clip[i,:,:,:] = temp\n",
    "    \n",
    "#average over first few stims to get auditory response to use for seeding\n",
    "dFF_aud_avg = np.mean(np.mean(dFF_aud_clip[0:4,:,:,:], axis = 0),axis = 0)\n",
    "\n",
    "\n",
    "fig, ax = subplots()\n",
    "avg = plt.imshow(dFF_aud_avg, vmin = np.min(dFF_aud_avg), vmax = 0, cmap = 'viridis')\n",
    "fig.colorbar(avg)\n",
    "fig.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "seedx = 185\n",
    "seedy = 45\n",
    "\n",
    "corr=np.full(avg_dFF.shape, np.nan)\n",
    "\n",
    "for allx in range(int(avg_dFF.shape[0])):\n",
    "    for ally in range(int(avg_dFF.shape[1])):\n",
    "        if all(dFF_blue[:,allx, ally] == dFF_blue[:,allx, ally]):\n",
    "            corr[allx,ally] = pearsonr(dFF_blue[:,seedx,seedy], dFF_blue[:,allx, ally])[0]\n",
    "            \n",
    "plt.imshow(corr)\n",
    "\n",
    "os.chdir(tif_folder)\n",
    "tifffile.imwrite('dFF_aud_avg.tif', dFF_aud_avg.astype('float32'))\n",
    "tifffile.imwrite('corr_audSeed.tif', corr.astype('float32'))\n",
    "\n",
    "#ImageJ\n",
    "# image transform\n",
    "# edit invert\n",
    "#auto\n",
    "# image threshold - manually adjust, apply, create mask, create outlines, select blood vessel image, shift-e, flatten, repeat with other modalities, save, add _MMM to name..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uobrainflex",
   "language": "python",
   "name": "uobrainflex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
